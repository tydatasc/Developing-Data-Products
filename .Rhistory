install.packages(swirl)
installed.packages()
install.packages("swirl")
install.packages("ggplot2")
install.packages("manipulate")
library(MASS)
dim(shuttle)
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888
fit
odds
fit <- glm(count ~ as.factor(spray) + offset(log(count+1)),
)
fit <- glm(count ~ as.factor(spray) + offset(log(count+1)), family="poisson", data=InsectSprays)
fit2 <- glm(count ~ as.factor(spray) + offset(log(10)+log(count+1)), family="poisson", data=InsectSprays)
summary(fit)$coef
summary(fit2)$coef
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54,
49        3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
install.packages("caret")
library(caret)
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
random forests, which add an additional layer of randomness to bagging. In addition to constructing each tree using a different bootstrap sample of the data, random forests change how the classiﬁcation or regression trees are constructed. In standard trees, each node is split using the best split among all variables. In a random forest, each node is split using the best among a subset of predictors randomly chosen at that node. This somewhat counterintuitive strategy turns out to perform very well compared to many other classiﬁers, including discriminant analysis, support vector machines and neural networks, and is robust against overﬁtting. In addition, it is very user-friendly in the sense that it has only two parameters (the number of variables in the random subset at each node and the number of trees in the forest), and is usually not very sensitive to their values.
random forests, which add an additional layer of randomness to bagging. In addition to constructing each tree using a different bootstrap sample of the data, random forests change how the classiﬁcation or regression trees are constructed. In standard trees, each node is split using the best split among all variables. In a random forest, each node is split using the best among a subset of predictors randomly chosen at that node. This somewhat counterintuitive strategy turns out to perform very well compared to many other classifiers, including discriminant analysis, support vector machines and neural networks, and is robust against overfitting. In addition, it is very user-friendly in the sense that it has only two parameters (the number of variables in the random subset at each node and the number of trees in the forest), and is usually not very sensitive to their values.
node and the number of trees in the forest), and is usually not very sensitive to their values.
library(ROCR)
# need to install and load the `devtools` package first
install.packages('devtools')
require(devtools)
# install rCharts
install_github('ramnathv/rCharts')
# install slidify
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
require(devtools)
install.packages("shiny")
mtcars
df <- mtcars
View(df)
shiny::runApp('GitHub/Developing-Data-Products')
shiny::runApp('GitHub/Developing-Data-Products')
setwd("~/GitHub/Developing-Data-Products")
shiny::runApp()
max(df)
